# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1njNSAG8pM_3wUgxoaxGsKMK85rN9OLDR
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 1. Load Dataset
df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
print("Dataset Shape:", df.shape)
print(df.head())

# 2. Data Cleaning

# Drop columns only if they exist in the dataset
cols_to_drop = ["EmployeeNumber", "Over18", "StandardHours"]
df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors="ignore")

# Encode categorical variables
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for col in df.select_dtypes(include="object").columns:
    df[col] = le.fit_transform(df[col].astype(str))  # convert to string in case of mixed types

# 3. Exploratory Data Analysis
plt.figure(figsize=(6,4))
sns.countplot(x="Attrition", data=df)
plt.title("Attrition Count")
plt.show()

plt.figure(figsize=(8,5))
sns.barplot(x="Department", y="Attrition", data=df)
plt.title("Attrition by Department")
plt.show()

# 4. Train-Test Split
X = df.drop("Attrition", axis=1)
y = df["Attrition"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 5. Logistic Regression Model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)

print("\nLogistic Regression Accuracy:", accuracy_score(y_test, y_pred_log))
print(confusion_matrix(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))

# Extract Logistic Regression coefficients
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': log_reg.coef_[0]
}).sort_values(by="Coefficient", ascending=False)

print("\nTop Logistic Regression Coefficients:")
print(coefficients.head(10))
print(coefficients.tail(10))

# Plot top positive & negative drivers
top_pos = coefficients.head(10)
top_neg = coefficients.tail(10)

plt.figure(figsize=(10,6))
sns.barplot(x="Coefficient", y="Feature", data=pd.concat([top_pos, top_neg]))
plt.title("Top Drivers of Attrition (Logistic Regression)")
plt.show()

# 6. Random Forest Model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("\nRandom Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

# Feature Importance from Random Forest
importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nTop Random Forest Features:")
print(importances.head(10))

# Plot feature importances
plt.figure(figsize=(10,6))
sns.barplot(x="Importance", y="Feature", data=importances.head(10))
plt.title("Top Features Driving Attrition (Random Forest)")
plt.show()